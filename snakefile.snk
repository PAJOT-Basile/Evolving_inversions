#### Libraries ####
import os
import pandas as pd
import numpy as np

######################## Configuration files ###############################

#configfile: "config_files/config_params.yaml"
#profile: "config_files"

######################## Import input files  ###############################

raw_data_path = config["raw_data_path"]
outputs_files = config["outputs_files"]
scratch_path = config["scratch_path"]
final_output = config["final_output"]
input_reference_genome = config["input_reference_genome"]

######################## Global variables  ###############################

READS = ["R1", "R2"]

########################  Functions   ###############################
######################## FASTQ Files  ###############################
def list_samples(directory, patterns_in, patterns_out):
    """
    This function returns all the sample names in the given directory

    Parameters
    ---------------------------
    directory: str
        This argument is the name of the directory to extract the names of the samples from
    
    patterns_in: list
        This list gives some filtering information on the files to use
    
    patterns_out: list 
        This list gives the patterns to not get in the files to use if any
    
    Returns
    ---------------------------
    SAMPLES: list
    The returned value is the list of all the samples extracted from the input directory
    """
    print(f"Patterns to keep in the filenames: {patterns_in}")
    print(f"Patterns to filter out in the filenames: {patterns_out}")
    # we make a list of the files in the raw data folder
    list_raw_data = pd.Series(os.listdir(directory))
    print(patterns_in)
    # we extract these files using the patterns parameters
    if patterns_in != "None":
        patterns_in.append("R1")
    else:
        patterns_in = ["R1"]
    list_of_samples = list_raw_data[list_raw_data.apply(lambda filter: all(word in filter for word in patterns_in))].sort_values()

    if patterns_out != "None":
        list_of_samples = pd.Series([sample for sample in list_of_samples for pattern in patterns_out if pattern not in sample])           
    # We then remove the end part of the file name
    list_of_samples = list(list_of_samples.str.replace(".R1.fastq.gz", ""))
    return(list_of_samples)

patterns_in = config["patterns_in"]
patterns_out = config["patterns_out"]

#SAMPLES = [list_samples(raw_data_path, patterns_in, patterns_out)[i] for i in (0, -1)]
SAMPLES = list_samples(raw_data_path, patterns_in, patterns_out)
#print(SAMPLES)

######################## Name of the reference genome  ###############################

def get_reference_genome_name(ref_path):
    """
    This function returns the name of the reference genome according to the specified path to said genome

    Parameters:
    -----------------------------------
    ref_path: str
        The path to the reference genome
    
    Returns:
    ------------------------------------
    name_genome: str
    The name of the reference genome
    """
    name_genome = ref_path.split("/")[-1]
    return(name_genome)

NAME_GENOME = get_reference_genome_name(input_reference_genome)   
reference_genome = final_output + "Reference/" + NAME_GENOME


######################## Index reference genome  ###############################
# Make a function that indexes the reference genome
def index_ref_genome(input_reference_genome, reference_genome):
    """
    This function indexes the reference genome to be used later in the workflow

    Parameters:
    ------------------------------------
    input_reference_genome: str
        This is the path to the input reference genome, whether it has been indexed already or not.
        It is the absolute path to the genome in fa format.
    
    reference_genome: str
        This is the path to the output reference genome that will be used for our analysis.

    Returns:
    ------------------------------------
    None: This function copies and/or indexes the genome so outputs are visible in the arborescence, but not in the console.
    """
    output_path = "/".join(reference_genome.split("/")[:-1]) + "/"
    GENOME_NAME = get_reference_genome_name(input_reference_genome)
    if not os.path.isfile(output_path + GENOME_NAME + ".amb"):
        print(f"Indexing reference genome: {GENOME_NAME}")
    os.popen(
        f"""
            if [ ! -d '{output_path}' ]; then
                mkdir -p {output_path}
            fi
            if [ ! -f "{output_path + get_reference_genome_name(input_reference_genome)}" ]; then
                if [ -f "{input_reference_genome}.amb" ]; then
                    cp {input_reference_genome}* {output_path}
                else
                    cp {input_reference_genome} {output_path}
                    bwa index {output_path}
                fi
            fi
        """
    )

index_ref_genome(input_reference_genome, reference_genome)
os.wait()
######################## Cut chromosomes  ###############################

# Make a function to separate the chromosomes into braks of a chosen length
def get_chromosome_positions_breaks(path, bin_size=1e6):
    """
    This function is used to cut every chromosome in the reference file index and separates it into bins of chosen width

    Parameters
    -----------------------------------
    path: str
        This is the path to the reference genome file (the fasta file). It requires that we have run the
        indexation of the reference genome beforehand.

    bin_size: int (default=1e6)
        This is the size of the bins to use to cut the chromosomes in samples of this length
        This parameter is optional and the default value is of 1e6

    
    Returns
    -------------------------------------
    list_positions: list of strings
    The returned values are the names of the chromosome and the bins in said chromosomes. For
    example, for a chromosome of size 2e6, it returns the following list: ['Chrom_name:1e6', 'Chrom_name:2e6']
    """
    # Initialise the list to add all the lines from the annotated reference genome file
    data = []
    with open(path + ".ann", "r") as line:
        data = line.readlines()
    
    # We remove the first line of the file given it does not provide any useful information here
    data.pop(0)

    # We separate the lines containing the length of the chromosomes (odd lines) from the names of the 
    # chromosomes (pair lines)
    data_pair = [0] * int(len(data)/2)
    data_odd = [0] * int(len(data)/2)
    for count, i in enumerate(data):
        if count % 2 != 0:
            data_odd[int(count/2)] = i.strip("\n")
        else:
            data_pair[int(count/2)] = i.strip("\n")

    # We extract the lengths of the chromosomes from the odd lines
    lengths = []
    for count, i in enumerate(data_odd):
        lengths.append(i.split(" ")[1])
    # We transform the lengths list into a list of integers to use as numbers
    lengths = list(map(int, lengths))

    # We extract the names of the chromosomes from the even lines
    names = []
    for count, i in enumerate(data_pair):
        names.append(i.split(" ")[1])

    # We concatenate the two lists into a dictionnary
    concat_dict = dict(map(lambda i,j: (i, j), names, lengths))

    # We make a list of the names of the chromosomes with positions added to it
    list_postions = []
    # On itÃ¨re sur les noms de chromosomes et le longueurs de chromosomes
    for key, value in concat_dict.items():
        # If the chromosome is longer than the chosen bin width, we cut it into the required number of bins using
        # the bin width we want to use  
        if value > bin_size:
            positions = np.arange(start=0, stop=value, step=bin_size)
            # Once the chromosome is cut into different samples, we iterate over the samples to add them to the 
            # list of positions 
            for count, i in enumerate(positions):
                # For the last sample, if the number of nucleotides is bigger than the length of the chromosome,
                # we add the remaining number of values
                if count == len(positions)-1:
                    list_postions.append(key + ":" + str(int(i)+1) + "-" + str(int(value)))
                else:
                    list_postions.append(key + ":" + str(int(i)+1) + "-" + str(int(positions[count+1])))
        else:
            list_postions.append(key + ":1-" + str(int(value)))
    
    return(list_postions)

REGIONS = get_chromosome_positions_breaks(reference_genome, bin_size=1e7)
















######################## RULES  ###############################
######################## rule all  ###############################
# Allows to check for input and outputs
rule all:
    input:
        ".mkdir.done",
        expand(final_output + "Fastqc_out/{sample}.{read}_fastqc.html", sample=SAMPLES, read=READS),
        #final_output + "MultiQC/Quality_results.html",
        expand(final_output + "Fastp/html/{sample}.html", sample=SAMPLES),
        expand(final_output + "Fastp/json/{sample}.json", sample=SAMPLES),
        expand(final_output + "Marked_duplicates/{sample}.cram", sample=SAMPLES),
        expand(final_output + "Marked_duplicates/{sample}.cram.bai", sample=SAMPLES),
        expand(final_output + "Flagstat_reports/{sample}.flagstat", sample=SAMPLES),
        final_output + "Stats_VCF/Number_SNPs_per_region.csv",
        final_output + "Full_VCF/Variant_calling_with_ref_genome.vcf.gz",
        final_output + "Filtered_VCF/Removed_indels_and_multiallelic_sites.vcf.gz",
        final_output + "Stats/DP/vcfstats.DP.txt",
        final_output + "Stats/MQ/vcfstats.MQ.txt",
        final_output + "Stats/QUAL/vcfstats.QUAL.txt",
        final_output + "Stats/SP/vcfstats.SP.txt",
        final_output + "Stats/AF/vcfstats.AF.txt"


######################## Create arborescence  ###############################
rule Num_01_Create_Arborescence:
    input:
        final_out = final_output
    output:
        indexer = directory(".mkdir.done"),
        fake_out = os.getcwd() +
    params:
        final_output + "Fastqc_out/",
        outputs_files + "Fastqc_out/",
        final_output + "MultiQC/",
        outputs_files + "Fastp/",
        final_output + "Fastp/html/",
        final_output + "Fastp/json/",
        outputs_files + "Mapped_genomes/",
        outputs_files + "Sorted_genomes/",
        final_output + "Marked_duplicates/",
        final_output + "Flagstat_reports/",
        outputs_files + "Concatenation/",
        outputs_files + "VCF_files/",
        outputs_files + "Stats_VCF",
        outputs_files + "Stats/DP/",
        outputs_files + "Stats/MQ/",
        outputs_files + "Stats/QUAL/",
        outputs_files + "Stats/SP/",
        outputs_files + "Stats/AF/",
        final_output + "Stats_VCF/",
        final_output + "Full_VCF/",
        final_output + "Filtered_VCF/",
        final_output + "Stats/DP/",
        final_output + "Stats/MQ/",
        final_output + "Stats/QUAL/",
        final_output + "Stats/SP/",
        final_output + "Stats/AF/",
    shell:
        """
            mkdir -p {output:q} {params:q}
        """


######################## Run Fastqc on raw data  ###############################
rule Num_02_FastQC:
    input:
        fake = rules.Num_01_Create_Arborescence.output.fake_out,
        real = raw_data_path + "{sample}.{read}.fastq.gz"
    output:
        zip_out = temp(outputs_files + "Fastqc_out/{sample}.{read}_fastqc.zip"),
        html_out = outputs_files + "Fastqc_out/{sample}.{read}_fastqc.html"
    message:
        "Processing {wildcards.sample}.{wildcards.read} in FastQC"
    shell :
        """
            OUTDIR="{config[outputs_files]:q}Fastqc_out/"
            fastqc {input.real:q} -o $OUTDIR
        """


######################## Run MultiQC on fastqc output  ###############################
rule Num_03_MultiQC:
    input:
        zip = expand(rules.Num_02_FastQC.output.zip_out, sample=SAMPLES, read=READS),
        html = expand(rules.Num_02_FastQC.output.html_out, sample=SAMPLES, read=READS),
    output:
        Multiqc = final_output + "MultiQC/Quality_results.html",
        fastqc = expand(final_output + "Fastqc_out/{sample}.{read}_fastqc.html", sample=SAMPLES, read=READS)
    params:
        INDIR = outputs_files + "Fastqc_out/",
        OUTNAME = final_output + "MultiQC/Quality_results"
    resources:
        mem_gb=config["mem_multiqc"]
    message:
        "Quality control with MultiQC"
    shell:
        """
            OUTDIR="{config[final_output]:q}MultiQC/"
            mkdir -p $OUTDIR
            multiqc {params.INDIR:q} -o $OUTDIR -n {params.OUTNAME:q}
            mkdir -p {config[outputs_files]}/Fastqc_out/
            mv "{config[outputs_files]}Fastqc_out/*.html" "{config[final_output]}Fastqc_out/"
        """


######################## Run FastP on raw files  ###############################
rule Num_04_FastP:
    input:
        #fake2 = rules.Num_03_MultiQC.output.Multiqc,
        fake = rules.Num_01_Create_Arborescence.output,
        raw_R1 = raw_data_path + "{sample}.R1.fastq.gz",
        raw_R2 = raw_data_path + "{sample}.R2.fastq.gz"
    output:
        fastp_R1 = temp(outputs_files + "Fastp/{sample}_R1.fastq.gz"),
        fastp_R2 = temp(outputs_files + "Fastp/{sample}_R2.fastq.gz"),
        html = final_output + "Fastp/html/{sample}.html",
        json = final_output + "Fastp/json/{sample}.json"
    threads: 4
    message:
        "Processing {wildcards.sample} in FastP"
    shell:
        """
            fastp -i {input.raw_R1:q} -I {input.raw_R2:q} -o {output.fastp_R1:q} -O {output.fastp_R2:q} --thread {threads} -g -c -y 30 --html {output.html:q} --json {output.json:q}
        """


######################## Map on the reference genome  ###############################
rule Num_05_Map_ref_genome:
    input:
        trimmed_R1 = rules.Num_04_FastP.output.fastp_R1,
        trimmed_R2 = rules.Num_04_FastP.output.fastp_R2,
        reference_genome_indexed = reference_genome + ".amb",
        ref_genome = reference_genome
    output:
        temp(outputs_files + "Mapped_genomes/{sample}.cram")
    threads: 10
    resources:
        partition="long",
        mem_gb=32
    message:
        "Mapping {wildcards.sample} on {NAME_GENOME}"
    shell:
        r"""
            bwa mem -M -t {threads} -R '@RG\tID:1\tSM:{wildcards.sample}\tPL:ILLUMINA\tLB:lib\tPU:transect' {input.ref_genome:q} {input.trimmed_R1:q} {input.trimmed_R2:q} | samtools view -C -T {input.ref_genome:q} > {output:q}
        """


######################## Sort the CRAM files  ###############################
rule Num_06_Sort_CRAM:
    input:
        rules.Num_05_Map_ref_genome.output
    output:
        temp(outputs_files + "Sorted_genomes/{sample}.cram")
    threads: 10
#    resources:
#        mem_mb=get_mem_mb_sort
    message:
        "Sorting {wildcards.sample}"
    shell:
        """
            samtools collate -@ {threads} -Ou {input:q} | samtools fixmate -@ {threads} -m - - | samtools sort -@ {threads} - -o {output:q}
        """


######################## Mark the duplicates  ###############################
rule Num_07_Mark_duplicates:
    input:
        rules.Num_06_Sort_CRAM.output
    output:
        final_output + "Marked_duplicates/{sample}.cram"
    threads: 10
    message:
        "Marking duplicates for {wildcards.sample}"
    shell:
        """
            samtools markdup -@ {threads} -d 2500 {input:q} {output:q}
        """


######################## Index the CRAM file and do some stats  ###############################
rule Num_08_Index_Flagstate:
    input:
        rules.Num_07_Mark_duplicates.output
    output:
        index = final_output + "Marked_duplicates/{sample}.cram.bai",
        flagstat = final_output + "Flagstat_reports/{sample}.flagstat"
    threads: 10
    message:
        "Indexing and making stats on {wildcards.sample}"
    shell:
        """
            samtools index @ {threads} -b {input:q} > {output.index:q} 
            samtools flagstat @ {threads} {input:q} > {output.flagstat:q}
        """


######################## Make a list of the CRAM files  ###############################
rule Num_09_Create_list_CRAM_files:
    input:
        real = expand(final_output + "Marked_duplicates/{sample}.cram", sample=SAMPLES),
        fake = expand(final_output + "Marked_duplicates/{sample}.cram.bai", sample=SAMPLES)
    output:
        temp(outputs_files + "Concatenation/List_cram_files.txt")
    shell:
        """
            LIST_DIR={config[final_output]}Marked_duplicates/*
            ls -d $LIST_DIR | grep -v ".bai" > {output:q}
        """


######################## Variant Calling  ###############################
rule Num_10_Compile_CRAM_files:
    input:
        ref_genome = reference_genome,
        list_cram_files = rules.Num_09_Create_list_CRAM_files.output,
        fake = expand(rules.Num_08_Index_Flagstate.output.index, sample=SAMPLES)
    output:
        temp(outputs_files + "VCF_files/VCF_File_{region}.vcf.gz")
    threads: 10
    params:
        region = expand("{region}", region=REGIONS)
    message:
        "VCF file preparation in region: {wildcards.region}"
    shell:
        """
            bcftools mpileup --threads {threads} -a FORMAT/AD,FORMAT/DP,FORMAT/SP,INFO/AD --fasta-ref {input.ref_genome:q} -b {input.list_cram_files:q} --regions {wildcards.region} | bcftools call --threads {threads} -m -Oz -o {output:q}
        """


######################## Count the number of SNPs in the file part 1  ###############################
rule Num_11_Count_SNPs:
    input:
        outputs_files + "VCF_files/VCF_File_{region}.vcf.gz",
    output:
        temp(outputs_files + "Stats_VCF/Number_SNPs_{region}.csv")
    shell:
        """
            NUM_SNPs=$(echo "$(zcat {input:q} | grep -v '#' | wc -l) + 1" | bc)
            echo "{wildcards.region};$(echo "$NUM_SNPs -1 " | bc)" >> {output:q}
        """


######################## Count the number of SNPs in the file part 2  ###############################
rule Num_11_Count_SNPs_2:
    input:
        temporary = expand(outputs_files + "Stats_VCF/Number_SNPs_{region}.csv", region=REGIONS),
    output:
        final_output + "Stats_VCF/Number_SNPs_per_region.csv",
    shell:
        """
            cat {input.temporary:q} >> {output:q}
        """


######################## Concatenate the VCF files  ###############################
rule Num_12_Concatenate_VCFs:
    input:
        expand(outputs_files + "VCF_files/VCF_File_{regions}.vcf.gz", regions=REGIONS)
    output:
        temp_out = temp(final_output + "Full_VCF/Variant_calling_with_ref_genome.vcf"),
        real_out = final_output + "Full_VCF/Variant_calling_with_ref_genome.vcf.gz"
    message:
        "Concatenating VCF files"
    shell:
        """
            zcat {input[0]:q} | grep "#" > {output.temp_out:q}
            zcat {input:q} | grep -v "#" >> {output.temp_out:q}
            tar -czf {output.real_out:q} {output.temp_out:q}
        """


######################## Remove Indels and multiallelic sites  ###############################
rule Num_13_Remove_Indels:
    input:
        rules.Num_10_Compile_CRAM_files.output
    output:
        temp(outputs_files + "Removed_indels/SNP_only_{region}.vcf.gz")
    threads: 10
    message:
        "Removing Indels and multiallelic sites for {wildcards.region}"
    shell:
        """
            bcftools filter -Ou --threads {threads} -g 5:indel,other {input:q} | bcftools view -Oz --threads {threads} -M 2 -m 2 -v snps > {output:q}
        """


######################## Concatenating filtered VCFs  ###############################
rule Num_14_VCF_Concat_after_filtering:
    input:
        expand(rules.Num_13_Remove_Indels.output, region=REGIONS)
    output:
        temp_out = temp(outputs_files + "Filtered_VCF/Removed_indels_and_multiallelic_sites.vcf"),
        real_out = final_output + "Filtered_VCF/Removed_indels_and_multiallelic_sites.vcf.gz"
    message:
        "Concatenating filtered VCFs"
    shell:
        """
            zcat {input[0]:q} | grep "#" > {output.temp_out:q}
            zcat {input:q} | grep -v "#" >> {output.temp_out:q}
            tar -czf {output.real_out:q} {output.temp_out:q}
        """


######################## Stats  ###############################
rule Num_15_Stats_Filtering:
    input:
        rules.Num_13_Remove_Indels.output,
    output:
        tot_depth = temp(outputs_files + "Stats/DP/depth_stats_{region}.txt"),
        map_qual = temp(outputs_files + "Stats/MQ/map_q_{region}.txt"),
        site_qual = temp(outputs_files + "Stats/QUAL/site_qual_{region}.txt"),
        phred = temp(outputs_files + "Stats/SP/phred_qual_{region}.txt"),
        allel_freq = temp(outputs_files + "Stats/AF/allel_freq_{region}.txt"),
    threads: 10
    message:
        "Doing stats after removing indels on {wildcards.region}"
    shell:
        r"""
            # Total depth read
            bcftools query --threads {threads} -f "%CHROM\t%POS\t%DP\t]\n" {input:q} > {output.tot_depth:q}

            # Map quality per site
            bcftools query --threads {threads} -f "%CHROM\t%POS\t%MQ\n" {input:q} > {output.map_qual:q}
            
            # Call quality per site
            bcftools query --threads {threads} -f "%CHROM\t%POS\t%QUAL\n" {input:q} > {output.site_qual:q}

            # Strand-bias P-value (Phread score)
            bcftools query --threads {threads} -f "%CHROM\t%POS\t[%SP\t]\n" {input:q} | awk 'BEGIN{{OFS="\t"}}{{sum=0; for (i=3; i<=NF; i++) sum+=$i; sum/=NF; print $1,$2,sum}}' > {output.phred:q}

            # Depth per sample
            bcftools +fill-tags --threads {threads} {input:q} -- -t AF | bcftools query -f "%CHROM\t%POS\t%AF\n" > {output.allel_freq:q}
        """


######################## Concat Stats  ###############################
rule Num_16_Concat_Stats:
    input:
        tot_depth = expand(outputs_files + "Stats/DP/depth_stats_{region}.txt", region=REGIONS),
        map_qual = expand(outputs_files + "Stats/MQ/map_q_{region}.txt", region=REGIONS),
        site_qual = expand(outputs_files + "Stats/QUAL/site_qual_{region}.txt", region=REGIONS),
        phred = expand(outputs_files + "Stats/SP/phred_qual_{region}.txt", region=REGIONS),
        allel_freq = expand(outputs_files + "Stats/AF/allel_freq_{region}.txt", region=REGIONS),
    output:
        tot_depth = final_output + "Stats/DP/vcfstats.DP.txt", 
        map_qual = final_output + "Stats/MQ/vcfstats.MQ.txt", 
        site_qual = final_output + "Stats/QUAL/vcfstats.QUAL.txt", 
        phred = final_output + "Stats/SP/vcfstats.SP.txt", 
        allel_freq = final_output + "Stats/AF/vcfstats.AF.txt"
    message:
        "Concatenating Stats"
    shell:
        """
            cat {input.tot_depth:q} > {output.tot_depth:q}
            cat {input.map_qual:q} > {output.map_qual:q}
            cat {input.site_qual:q} > {output.site_qual:q}
            cat {input.phred:q} > {output.phred:q}
            cat {input.allel_freq:q} > {output.allel_freq:q}
        """
